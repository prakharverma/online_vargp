{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import online_gp\n",
    "import gpytorch\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2.0)\n",
    "\n",
    "label_fs = 40\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(\"bright\")\n",
    "\n",
    "palette = sns.color_palette(\"Paired\", 10)\n",
    "palette.reverse()\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette[0] = \"#57068c\"\n",
    "palette[-2] = \"#28619e\"\n",
    "palette[-1] = \"#3dbbdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from online_gp.models import VariationalGPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpytorch.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fx_rawdata = pd.read_csv('https://raw.githubusercontent.com/trungngv/cogp/master/data/fx/fx2007-processed.csv',\n",
    "                        header=None)\n",
    "\n",
    "inputs = torch.arange(0, fx_rawdata[3].shape[0]).view(-1,1).float()\n",
    "targets = torch.from_numpy(fx_rawdata[3].values).float() \n",
    "\n",
    "inputs, targets = inputs[:60], targets[:60]\n",
    "\n",
    "tmean = targets.mean()\n",
    "tstd = targets.std()\n",
    "targets = (targets - tmean) / tstd\n",
    "\n",
    "imean = inputs.mean()\n",
    "istd = inputs.std()\n",
    "inputs = (inputs - imean) / istd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(mll, model, optimizer, x, y, num_steps=1000, verbose=True):\n",
    "    for i in range(num_steps):\n",
    "        loss = -mll(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % (num_steps // 10 if num_steps > 10 else 1) == 0 and verbose:\n",
    "            print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_basic_plot(model, x, y, old_x=None, old_y=None, bounds=(-6., 6.), col = -2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x = torch.linspace(*bounds).view(-1,1)\n",
    "        pred_dist = vargp_model(test_x)\n",
    "        pred_induc = vargp_model(vargp_model.variational_strategy.inducing_points.data.view(-1,1))\n",
    "        \n",
    "    plt.plot(test_x, pred_dist.mean, label = \"Predictive Mean\", color = palette[col], linewidth=4, zorder=3)\n",
    "    plt.fill_between(\n",
    "        test_x.view(-1), *[x.detach() for x in pred_dist.confidence_region()], alpha = 0.3, color = palette[col],\n",
    "        zorder=6\n",
    "    )\n",
    "    \n",
    "    plt.scatter(x, y, color = \"#d71e5e\", label = \"Current Data\", marker = \"x\", s=100, zorder=20)\n",
    "    plt.scatter(vargp_model.variational_strategy.inducing_points.data, pred_induc.mean.detach(), \n",
    "            color = \"#220337\", marker=\"*\", label = \"Inducing Points\", s=150, zorder=15)\n",
    "    if old_x is not None:\n",
    "        plt.scatter(\n",
    "            old_x, old_y, color = \"#d71e5e\", marker = \"x\", alpha = 0.3, s=100, label = \"Old Data\", zorder=15)\n",
    "        \n",
    "    # plt.legend()\n",
    "    plt.xlabel(\"x\", fontsize = 16)\n",
    "    plt.ylabel(\"y\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit in Online Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from gpytorch.lazy import LazyTensor\n",
    "from typing import Union\n",
    "\n",
    "def _pivoted_cholesky_init(\n",
    "    train_inputs: Tensor,\n",
    "    kernel_matrix: Union[Tensor, LazyTensor],\n",
    "    max_length: int,\n",
    "    epsilon: float = 1e-10,\n",
    ") -> Tensor:\n",
    "    r\"\"\"\n",
    "    A pivoted cholesky initialization method for the inducing points, originally proposed in\n",
    "    [burt2020svgp] with the algorithm itself coming from [chen2018dpp]. Code is a PyTorch version from\n",
    "    [chen2018dpp], copied from https://github.com/laming-chen/fast-map-dpp/blob/master/dpp.py.\n",
    "    Args:\n",
    "        train_inputs [Tensor]: training inputs\n",
    "        kernel_matrix [Tensor or Lazy Tensor]: kernel matrix on the training inputs\n",
    "        max_length [int]: number of inducing points to initialize\n",
    "        epsilon [float]: numerical jitter for stability.\n",
    "    \"\"\"\n",
    "    # this is numerically equivalent to iteratively performing a pivoted cholesky\n",
    "    # while storing the diagonal pivots at each iteration\n",
    "    # TODO: use gpytorch's pivoted cholesky instead once that gets an exposed list\n",
    "    # TODO: this probably won't work in batch mode.\n",
    "    item_size = kernel_matrix.shape[-2]\n",
    "    cis = torch.zeros((max_length, item_size))\n",
    "    di2s = kernel_matrix.diag()\n",
    "    selected_items = []\n",
    "    selected_item = torch.argmax(di2s)\n",
    "    selected_items.append(selected_item)\n",
    "    while len(selected_items) < max_length:\n",
    "        k = len(selected_items) - 1\n",
    "        ci_optimal = cis[:k, selected_item]\n",
    "        di_optimal = torch.sqrt(di2s[selected_item])\n",
    "        elements = kernel_matrix[..., selected_item, :]\n",
    "        eis = (elements - torch.matmul(ci_optimal, cis[:k, :])) / di_optimal\n",
    "        cis[k, :] = eis\n",
    "        di2s = di2s - eis.pow(2.0)\n",
    "        di2s[selected_item] = -(torch.tensor(float(\"inf\")))\n",
    "        selected_item = torch.argmax(di2s)\n",
    "        if di2s[selected_item] < epsilon:\n",
    "            break\n",
    "        selected_items.append(selected_item)\n",
    "    ind_points = train_inputs[torch.stack(selected_items)]\n",
    "    return ind_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=3)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "vargp_model = VariationalGPModel(\n",
    "    _pivoted_cholesky_init(inputs[:20], cm(inputs[:20]), 15), \n",
    "    streaming=False, \n",
    "    likelihood = likelihood,\n",
    "    covar_module = cm,\n",
    "    learn_inducing_locations=False,\n",
    ")\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood=likelihood, model=vargp_model, num_data=20, beta = 1.0)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(vargp_model.parameters()), lr = 1e-2)\n",
    "\n",
    "fit_model(mll, vargp_model, optimizer, inputs[:20], targets[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 3))\n",
    "make_basic_plot(vargp_model, inputs[:20], targets[:20], bounds=(-3., 3))\n",
    "# plt.savefig(\"./plots/osvgp_pivchol_full_t20.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "for i in range(20, 60, step):\n",
    "    print(\"Starting step: \", i)\n",
    "    next_x = inputs[i:i+step]\n",
    "    next_y = targets[i:i+step]\n",
    "    \n",
    "    with gpytorch.settings.cholesky_jitter(1e-3):\n",
    "        vargp_model.update_variational_parameters(\n",
    "            next_x, \n",
    "            next_y, \n",
    "            _pivoted_cholesky_init(inputs[:(i+step)], vargp_model.covar_module(inputs[:(i+step)]).add_jitter(1e-4), 15)\n",
    "        )\n",
    "    \n",
    "    vargp_model.zero_grad()\n",
    "    vargp_model.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(\n",
    "        likelihood=likelihood, \n",
    "        model=vargp_model, \n",
    "        num_data=step, \n",
    "        beta = 1.0,\n",
    "        combine_terms=True\n",
    "    )\n",
    "    \n",
    "    fit_model(mll, vargp_model, optimizer, next_x, next_y, num_steps=10, verbose=False)\n",
    "    \n",
    "    if i % 20 == 0 or i == 59:\n",
    "        plt.figure(figsize = (6,3))\n",
    "        make_basic_plot(\n",
    "            vargp_model, \n",
    "            next_x, \n",
    "            next_y, \n",
    "            old_x=inputs[:i], \n",
    "            old_y=targets[:i], \n",
    "            bounds=(-3., 3.)\n",
    "        )\n",
    "        plt.savefig(\"./plots/osvgp_pivchol_full_t\"+str(i)+\".pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit in Online Mode (Coreset-Like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=3)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "vargp_model = online_gp.models.VariationalGPModel(\n",
    "    _pivoted_cholesky_init(inputs[:20], cm(inputs[:20]), 15), \n",
    "    streaming=False, \n",
    "    likelihood = likelihood,\n",
    "    covar_module = cm,\n",
    "    learn_inducing_locations=False,\n",
    ")\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood=likelihood, model=vargp_model, num_data=20, beta = 1.0)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(vargp_model.parameters()), lr = 1e-2)\n",
    "\n",
    "fit_model(mll, vargp_model, optimizer, inputs[:20], targets[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "make_basic_plot(vargp_model, inputs[:20], targets[:20], bounds=(-3., 3), col = 0)\n",
    "plt.savefig(\"./plots/osvgp_pivchol_partial_t20.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "for i in range(20, 60, step):\n",
    "    print(\"Starting step: \", i)\n",
    "    next_x = inputs[i:i+step]\n",
    "    next_y = targets[i:i+step]\n",
    "    \n",
    "    with gpytorch.settings.cholesky_jitter(1e-3):\n",
    "        stacked_data = torch.cat((next_x, vargp_model.variational_strategy.inducing_points.detach()))\n",
    "        \n",
    "        vargp_model.update_variational_parameters(\n",
    "            next_x, \n",
    "            next_y, \n",
    "            _pivoted_cholesky_init(stacked_data, cm(stacked_data).add_jitter(1e-4), 15)\n",
    "        )\n",
    "    \n",
    "    vargp_model.zero_grad()\n",
    "    vargp_model.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(\n",
    "        likelihood=likelihood, \n",
    "        model=vargp_model, \n",
    "        num_data=step, \n",
    "        beta = 1.0,\n",
    "        combine_terms=True\n",
    "    )\n",
    "    \n",
    "    fit_model(mll, vargp_model, optimizer, next_x, next_y, num_steps=10, verbose=False)\n",
    "    \n",
    "    if i % 20 == 0 or i == 59:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        make_basic_plot(\n",
    "            vargp_model, \n",
    "            next_x, \n",
    "            next_y, \n",
    "            old_x=inputs[:i], \n",
    "            old_y=targets[:i], \n",
    "            bounds=(-3., 3.),\n",
    "            col=0\n",
    "        )\n",
    "        plt.savefig(\"./plots/osvgp_pivchol_partial_t\"+str(i)+\".pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random movement of inducing pts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=3)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "vargp_model = online_gp.models.VariationalGPModel(\n",
    "    _pivoted_cholesky_init(inputs[:20], cm(inputs[:20]), 15), \n",
    "    streaming=False, \n",
    "    likelihood = likelihood,\n",
    "    covar_module = cm,\n",
    "    learn_inducing_locations=False,\n",
    ")\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood=likelihood, model=vargp_model, num_data=20, beta = 1.0)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(vargp_model.parameters()), lr = 1e-2)\n",
    "\n",
    "fit_model(mll, vargp_model, optimizer, inputs[:20], targets[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "make_basic_plot(vargp_model, inputs[:20], targets[:20], bounds=(-3., 3), col = -1)\n",
    "plt.savefig(\"./plots/osvgp_none_t20.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(list(vargp_model.parameters()), lr = 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step = 1\n",
    "\n",
    "for i in range(20, 60, step):\n",
    "    print(\"Starting step: \", i)\n",
    "    next_x = inputs[i:i+step]\n",
    "    next_y = targets[i:i+step]\n",
    "    \n",
    "    rm = max(1, torch.randint(15, torch.Size()).item())\n",
    "    ind_pts = vargp_model.variational_strategy.inducing_points.clone().detach()\n",
    "    \n",
    "    new_inducing = torch.cat((\n",
    "        ind_pts[:rm-1],\n",
    "        ind_pts[rm:],\n",
    "        next_x.view(-1,1),\n",
    "        ))\n",
    "    # print(ind_pts.shape, new_inducing.shape, next_x.shape, rm)\n",
    "    with gpytorch.settings.cholesky_jitter(1e-3):       \n",
    "        vargp_model.update_variational_parameters(\n",
    "            next_x, \n",
    "            next_y, \n",
    "            new_inducing,\n",
    "        )\n",
    "    \n",
    "    vargp_model.zero_grad()\n",
    "    vargp_model.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(\n",
    "        likelihood=likelihood, \n",
    "        model=vargp_model, \n",
    "        num_data=step, \n",
    "        beta = 1.,\n",
    "        combine_terms=True\n",
    "    )\n",
    "    \n",
    "    fit_model(mll, vargp_model, optimizer, next_x, next_y, num_steps=10, verbose=False)\n",
    "    \n",
    "    if i % 20 == 0 or i == 59:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        make_basic_plot(\n",
    "            vargp_model, \n",
    "            next_x, \n",
    "            next_y, \n",
    "            old_x=inputs[:i], \n",
    "            old_y=targets[:i], \n",
    "            bounds=(-3., 3.),\n",
    "            col=-1\n",
    "        )\n",
    "        plt.savefig(\"./plots/osvgp_pivchol_none_t\"+str(i)+\".pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_basic_plot(\n",
    "    vargp_model, \n",
    "    next_x, \n",
    "    next_y, \n",
    "    old_x=inputs[:i], \n",
    "    old_y=targets[:i], \n",
    "    bounds=(-3., 3.),\n",
    "    col=-1\n",
    ")\n",
    "plt.legend(ncol = 4, loc = \"upper center\", bbox_to_anchor = (0.5, -0.2))\n",
    "# plt.savefig(\"plots/legend.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.rand(1), torch.rand(1), label = \"Resampled O-SGPR\", color = palette[-1], linewidth=4)\n",
    "plt.plot(torch.rand(1), torch.rand(1), label = \"Piv. Chol. O-SGPR\", color = palette[0], linewidth=4)\n",
    "plt.scatter(torch.rand(1), torch.rand(1), color = \"#d71e5e\", \n",
    "            label = \"Current Data\", marker = \"x\", s=100, zorder=20)\n",
    "plt.scatter(torch.rand(1), torch.rand(1), color = \"#d71e5e\", \n",
    "            label = \"Old Data\", marker = \"x\", s=100, zorder=20, alpha = 0.3)\n",
    "plt.scatter(torch.rand(1), torch.rand(1), color = \"#220337\", marker=\"*\",\n",
    "            label = \"Inducing Points\", s=150, zorder=20)\n",
    "plt.legend(ncol = 5, loc = \"upper center\", bbox_to_anchor = (0.5, -0.2))\n",
    "plt.savefig(\"sgpr_legend.pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda52eab690427c4f7ea56588deee120c46"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
